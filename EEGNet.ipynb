{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, SeparableConv2D, DepthwiseConv2D, BatchNormalization, AveragePooling2D, Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "#fixing seed\n",
    "tensorflow.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = r'/content/drive/My Drive/bci_iv_2a_data/Original/' #original data directory\n",
    "generated_data_dir = r'/content/drive/My Drive/bci_iv_2a_data/Generated/' #generated data directory\n",
    "\n",
    "mms = MinMaxScaler() #minmaxscaler\n",
    "\n",
    "train_dataset = []\n",
    "train_label = []\n",
    "\n",
    "generated_train_dataset = []\n",
    "generated_train_label = []\n",
    "\n",
    "test_dataset = []\n",
    "test_label = []\n",
    "\n",
    "\n",
    "# loading original EEG training dataset\n",
    "for f, folder in enumerate(os.listdir(direc)):\n",
    "    train_folder = glob.glob(direc + folder + '/train/')\n",
    "    for i in range(0,len(os.listdir(train_folder[0]))):\n",
    "        files = os.listdir(train_folder[0]+str(i))\n",
    "        for j, name in enumerate(files):\n",
    "            filename = glob.glob(train_folder[0]+str(i)+'/'+name)\n",
    "            df = pd.read_csv(filename[0], index_col=None, header=None)\n",
    "            df = df.drop(0, axis=1) #dropping column of eeg channel names\n",
    "            df = df.iloc[:,0:1000] #taking 1000 timesteps\n",
    "            df = pd.DataFrame(mms.fit_transform(df)) #min-max scaling\n",
    "            train_dataset.append(np.array(df))\n",
    "            train_label.append(i) #appending class labels\n",
    "            \n",
    "\n",
    "# loading original EEG test dataset\n",
    "for f, folder in enumerate(os.listdir(direc)):\n",
    "    test_folder = glob.glob(direc + folder + '/test/')\n",
    "    for i in range(0,len(os.listdir(test_folder[0]))):\n",
    "        files = os.listdir(test_folder[0]+str(i))\n",
    "        for j, name in enumerate(files):\n",
    "            filename = glob.glob(test_folder[0]+str(i)+'/'+name)\n",
    "            df = pd.read_csv(filename[0], index_col=None, header=None)\n",
    "            df = df.drop(0, axis=1) #dropping column of eeg channel names\n",
    "            df = df.iloc[:,0:1000] #taking 1000 timesteps\n",
    "            df = pd.DataFrame(mms.fit_transform(df)) #min-max scaling\n",
    "            test_dataset.append(np.array(df)) \n",
    "            test_label.append(i) #appending class labels\n",
    "            \n",
    "\n",
    "# loading generated EEG dataset\n",
    "for f, folder in enumerate(os.listdir(generated_data_dir)):\n",
    "    train_folder = glob.glob(generated_data_dir + folder + '/train/')\n",
    "    for i in range(0,len(os.listdir(train_folder[0]))):\n",
    "        files = os.listdir(train_folder[0]+str(i))\n",
    "        for j, name in enumerate(files):\n",
    "            filename = glob.glob(train_folder[0]+str(i)+'/'+name)\n",
    "            df = pd.read_csv(filename[0], index_col=None, header=None)\n",
    "            df = df.iloc[:,0:1000] #taking 1000 timesteps\n",
    "            df = pd.DataFrame(mms.fit_transform(df)) #min-max scaling\n",
    "            generated_train_dataset.append(np.array(df))\n",
    "            generated_train_label.append(i) #appending class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e914402",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.array(train_dataset)\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "generated_train_dataset = np.array(generated_train_dataset)\n",
    "generated_train_label = np.array(generated_train_label)\n",
    "\n",
    "test_dataset = np.array(test_dataset)\n",
    "test_label = np.array(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.expand_dims(train_dataset,axis=-1)\n",
    "train_label = to_categorical(train_label)\n",
    "\n",
    "generated_train_dataset = np.expand_dims(generated_train_dataset,axis=-1)\n",
    "generated_train_label = to_categorical(generated_train_label)\n",
    "\n",
    "test_dataset = np.expand_dims(test_dataset,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c023261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEGNet architecture\n",
    "def EEGNet(nb_classes, Channels = 64, Samples = 128, dropout_rate = 0.5, kernelLength = 64, F1 = 8, D = 2, F2 = 16):\n",
    "    \n",
    "    \"\"\"     \n",
    "      Ref: Lawhern, Vernon & Solon, Amelia & Waytowich, Nicholas & Gordon, Stephen & Hung, Chou & Lance, Brent. (2016). \n",
    "      EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. \n",
    "      Journal of Neural Engineering. 15. 10.1088/1741-2552/aace8c. \n",
    "\n",
    "      nb_classes         : number of classes to classify\n",
    "      Channels, Samples  : number of channels and time steps \n",
    "      dropout_rate       : dropout rate\n",
    "      kernelLength       : length of temporal convolution in first layer.    \n",
    "      F1, F2             : number of temporal filters (F1) and number of pointwise filters (F2).  \n",
    "      D                  : number of spatial filters\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    inputs   = Input(shape = (Channels, Samples, 1))\n",
    "\n",
    "    block1       = Conv2D(F1, (1, kernelLength), padding = 'same', input_shape = (Channels, Samples, 1))(inputs)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = DepthwiseConv2D((Channels, 1), depth_multiplier = D, depthwise_constraint = max_norm(1.))(block1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('elu')(block1)\n",
    "    block1       = AveragePooling2D((1, 4))(block1)\n",
    "    block1       = Dropout(dropout_rate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 16), padding = 'same')(block1)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling2D((1, 8))(block2)\n",
    "    block2       = Dropout(dropout_rate)(block2)\n",
    "        \n",
    "    flatten      = Flatten()(block2)\n",
    "    \n",
    "    dense        = Dense(nb_classes)(flatten)\n",
    "    softmax      = Activation('softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e279e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying EEGNet for both training with both original and generated dataset\n",
    "eegnet_original_data = eegnet_generated_data = EEGNet(4, Channels = 22, Samples = 500, dropout_rate = 0.2, kernelLength = 64, F1 = 8,  D = 2, F2 = 16)\n",
    "\n",
    "eegnet_original_data.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "eegnet_generated_data.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback\n",
    "callbacks = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          mode='min',\n",
    "                          patience = 30,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataset,train_label, test_size=0.2)\n",
    "\n",
    "history1  = eegnet_original_data.fit(X_train, y_train, epochs=150, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a77c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with generated data\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(generated_train_dataset,generated_train_label, test_size=0.2)\n",
    "\n",
    "history2  = eegnet_generated_data.fit(X_train2, y_train2, epochs=150, validation_data = (X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb004e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss curves\n",
    "loss1 = history1.history['loss']\n",
    "loss2 = history2.history['loss']\n",
    "epochs = range(1, len(loss1) + 1)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(epochs, loss1, 'r', label='loss with original training data')\n",
    "plt.plot(epochs, loss2, 'b', label='loss with generated training data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b79a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report with original training data and test data\n",
    "print(classification_report(test_label, np.argmax(eegnet_original_data.predict(test_dataset), axis = 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report with generated training data and test data\n",
    "print(classification_report(test_label, np.argmax(eegnet_generated_data.predict(test_dataset), axis = 1)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EEGNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
